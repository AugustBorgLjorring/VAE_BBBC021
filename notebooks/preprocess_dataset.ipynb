{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc26693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def crop_image(image: np.ndarray, pix: int = 2) -> np.ndarray:\n",
    "    return image[pix:-pix, pix:-pix, :]\n",
    "\n",
    "def normalize_image(image: np.ndarray) -> np.ndarray:\n",
    "    image = image.astype(np.float32)\n",
    "    min_vals = image.min(axis=(0, 1), keepdims=True)\n",
    "    max_vals = image.max(axis=(0, 1), keepdims=True)\n",
    "    range_vals = np.maximum(max_vals - min_vals, 1e-5)\n",
    "    return (image - min_vals) / range_vals\n",
    "\n",
    "h5_path = 'D:/BBBC021/BBBC021_dataset.h5'\n",
    "\n",
    "# Function to load the HDF5 file and benchmark the preprocessing\n",
    "with h5py.File(h5_path, 'r') as h5f:\n",
    "    images = h5f['images']\n",
    "    N = len(images)\n",
    "\n",
    "    print(f\"\\nBenchmarking {N} images with preprocessing...\")\n",
    "    start = time.time()\n",
    "    skipped = 0\n",
    "    for i in tqdm(range(N)):\n",
    "        img = crop_image(images[i])\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        img = normalize_image(img)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "    preprocess_time = time.time() - start\n",
    "    print(f\"Preprocessing time: {preprocess_time:.2f} seconds ({N - skipped} images)\")\n",
    "\n",
    "    print(f\"\\nBenchmarking {N} images with no preprocessing...\")\n",
    "    start = time.time()\n",
    "    for i in tqdm(range(N)):\n",
    "        _ = images[i]\n",
    "    no_preprocess_time = time.time() - start\n",
    "    print(f\"Raw load time: {no_preprocess_time:.2f} seconds ({N} images)\")\n",
    "\n",
    "    print(f\"\\nTime difference: {preprocess_time - no_preprocess_time:.2f} seconds\")\n",
    "    print(f\"Per-image overhead: {(preprocess_time - no_preprocess_time) / (N - skipped):.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Define your noisy long IDs\n",
    "noisy_ids = {\n",
    "    \"C11_s1_w127B03F50-0445-404D-8686-992152316A15\",\n",
    "    \"G02_s2_w17DA4AF6A-01FB-4517-9E5B-CCDD62A1D4B6\",\n",
    "    \"Week1_150607_B03_s2_w16BAE18E6-E27B-4683-90FA-581B8ADA6CB1\", # maybe\n",
    "    \"Week1_150607_E04_s4_w12BDBC4CA-CCA1-48C8-B4D5-F229EB266410\", # maybe\n",
    "    \"Week1_150607_E11_s1_w1F5053E7F-88CD-40F8-AAE0-0633FF5CBBA4\", # maybe\n",
    "    \"Week1_150607_E11_s2_w185FD4BFE-407C-43FA-8376-281D21993540\", # maybe\n",
    "    \"Week2_180607_E02_s3_w109D7263C-9A3C-4717-B6AA-BA1ECCF45A70\", # maybe\n",
    "    \"Week2_180607_F02_s4_w14EBE2925-CBA5-42C4-8853-A78BACC04D3C\",\n",
    "    \"Week3_290607_B11_s3_w1E2E13398-A442-40FE-AF91-FB9B55A9D664\",\n",
    "    \"Week5_130707_C11_s4_w1EBDD4E60-9887-4A72-BFD1-C3D6EB633BE4\",\n",
    "    \"Week5_130707_C11_s4_w116C137AD-BD10-4685-984A-A8E52DA612BA\",\n",
    "    \"Week5_130707_D02_s2_w1286A02BA-8BE0-4267-B919-E6223AB4F182\",\n",
    "    \"Week6_200607_C11_s3_w19357A9CA-9915-45A9-8F2E-09B6AE811524\",\n",
    "    \"Week6_200607_E02_s2_w13ED36977-F80B-40C5-8151-7CF6DB9782AE\",\n",
    "    \"Week6_200607_E11_s4_w1AABB4630-0530-43D7-B915-902AA6480CC3\",\n",
    "    \"Week7_230707_C06_s2_w1D292D8D5-F5A6-4CC9-9A28-D4713C8D8076\",\n",
    "    \"Week7_230707_E02_s2_w182895F08-11E3-4AB6-B2B8-1BEECE2862F4\",\n",
    "    \"Week7_230707_F02_s1_w176094EF3-CBD9-4F0C-8388-D0A55B172980\",\n",
    "    \"Week7_230707_G02_s2_w1B5CBE42C-5E9F-4F2C-8094-23EF0BF691AA\",\n",
    "    \"Week7_230707_G02_s3_w1D044621A-12B7-4CCC-AD48-B4EFC5C0D221\",\n",
    "    \"Week8_4sites_B02_s2_w14F7C0A4C-02B2-49C5-909F-37CB44AD0FDF\", # maybe\n",
    "    \"Week8_4sites_C09_s2_w14A741543-3F34-49DE-A541-EB80EA9D937D\",\n",
    "    \"Week8_4sites_E11_s3_w13E4113DB-0EB6-4ED5-BB6E-1E3EE3F5EE57\",\n",
    "    \"Week9_090907_C02_s2_w1A5369617-A74F-4F39-BF1F-D5834D20B5C0\",\n",
    "    \"Week9_090907_C11_s3_w195438B0C-C73B-471B-AF96-85FA31D58F37\", # maybe\n",
    "}\n",
    "\n",
    "input_path = 'D:/BBBC021/BBBC021_dataset.h5'\n",
    "output_path = 'D:/BBBC021/BBBC021_cleaned_preprocessed.h5'\n",
    "\n",
    "# Preprocessing functions\n",
    "def crop_image(image: np.ndarray, pix: int = 2) -> np.ndarray:\n",
    "    return image[pix:-pix, pix:-pix, :] if image.shape == (68, 68, 3) else None\n",
    "\n",
    "def normalize_image(image: np.ndarray) -> np.ndarray:\n",
    "    image = image.astype(np.float32)\n",
    "    min_vals = image.min(axis=(0, 1), keepdims=True)\n",
    "    max_vals = image.max(axis=(0, 1), keepdims=True)\n",
    "    range_vals = np.maximum(max_vals - min_vals, 1e-5)\n",
    "    return (image - min_vals) / range_vals\n",
    "\n",
    "def is_noisy(name: str, bad_ids: set) -> bool:\n",
    "    prefix = '_'.join(name.split('_')[:-1])  # Remove the last `_###` part\n",
    "    return prefix in bad_ids\n",
    "\n",
    "# Processing\n",
    "with h5py.File(input_path, 'r') as in_f, h5py.File(output_path, 'w') as out_f:\n",
    "    raw_images = in_f['images']\n",
    "    raw_names = in_f['image_names']\n",
    "    total = len(raw_images)\n",
    "\n",
    "    img_shape = (3, 64, 64)\n",
    "    img_dtype = np.float32\n",
    "\n",
    "    # Create resizable datasets in output file\n",
    "    image_dataset = out_f.create_dataset(\n",
    "        'images',\n",
    "        shape=(0, *img_shape),\n",
    "        maxshape=(None, *img_shape),\n",
    "        dtype=img_dtype,\n",
    "        chunks=(1, *img_shape),\n",
    "        compression='gzip'\n",
    "    )\n",
    "\n",
    "    name_dataset = out_f.create_dataset(\n",
    "        'image_names',\n",
    "        shape=(0,),\n",
    "        maxshape=(None,),\n",
    "        dtype=h5py.string_dtype(),\n",
    "        compression='gzip'\n",
    "    )\n",
    "\n",
    "    index = 0\n",
    "    for i in tqdm(range(total), desc=\"Processing Images\"):\n",
    "        name = raw_names[i].decode('utf-8')\n",
    "        if is_noisy(name, noisy_ids):\n",
    "            print(f\"Removed image: {name}\")\n",
    "            continue\n",
    "\n",
    "        img = raw_images[i]\n",
    "        img = crop_image(img)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = normalize_image(img)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        # Save to output dataset\n",
    "        image_dataset.resize(index + 1, axis=0)\n",
    "        name_dataset.resize(index + 1, axis=0)\n",
    "\n",
    "        image_dataset[index] = img\n",
    "        name_dataset[index] = name.encode('utf-8')\n",
    "        index += 1\n",
    "\n",
    "print(f\"\\nSaved {index} cleaned & preprocessed images to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009087fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Path to the cleaned + preprocessed dataset\n",
    "h5_path = 'D:/BBBC021/BBBC021_cleaned_preprocessed.h5'\n",
    "\n",
    "with h5py.File(h5_path, 'r') as h5f:\n",
    "    images = h5f['images']\n",
    "    names = h5f['image_names']\n",
    "\n",
    "    num_images = images.shape[0]\n",
    "    img_shape = images.shape[1:]\n",
    "    dtype = images.dtype\n",
    "\n",
    "    print(f\"Dataset loaded from: {h5_path}\")\n",
    "    print(f\"Number of images: {num_images}\")\n",
    "    print(f\"Image shape: {img_shape}\")\n",
    "    print(f\"Data type: {dtype}\")\n",
    "    print(f\"Sample names: {[names[i].decode('utf-8') for i in range(min(5, num_images))]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
